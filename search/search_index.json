{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K8sGPT Documentation","text":"<p>K8sGPT gives Kubernetes SRE superpowers to everyone</p> <p>The documentation provides the following</p> <ul> <li>Getting started: Guides to install and use K8sGPT</li> <li>Tutorials: End-to-end tutorials on specific use cases</li> <li>Reference: Specific documentation on the features</li> <li>Explanation: Additional explanations on the design and use of the CLI</li> </ul>"},{"location":"#documentation-enhancements","title":"Documentation enhancements","text":"<p>If anything is unclear please create an issue in the docs repository.</p>"},{"location":"explanation/caching/","title":"Caching","text":"<p>Remote caching is a mechanism used to store and retrieve frequently accessed data in a location separate from the primary system. In the context of <code>K8sGPT</code>, it allows users to offload cached data to a remote storage service, like AWS S3, rather than managing it on the local machine. This approach offers several benefits, such as reducing local storage requirements and ensuring cache persistence even when the local environment is updated or restarted.</p>"},{"location":"explanation/caching/#aws-s3-integration","title":"AWS S3 Integration","text":"<p>K8sGPT provides seamless integration with AWS S3, a widely adopted and reliable object storage service offered by Amazon Web Services. By leveraging this integration, users can take advantage of AWS S3's scalability, durability, and availability to store their cached data remotely.</p>"},{"location":"explanation/caching/#prerequisites","title":"Prerequisites","text":"<p>To use the remote caching feature with AWS S3 in K8sGPT, you need to have the following prerequisites set up:</p> <ul> <li> <p><code>AWS_ACCESS_KEY_ID</code>: An access key ID is required to authenticate with AWS S3 programmatically.</p> </li> <li> <p><code>AWS_SECRET_ACCESS_KEY</code>: The corresponding secret access key that pairs with the AWS access key ID.</p> </li> </ul> <p>Adding a Remote Cache:</p> <p>Users can easily add a remote cache to the K8sGPT CLI by executing the following command:</p> <pre><code>k8sgpt cache add --region &lt;aws region&gt; --bucket &lt;name&gt;\n</code></pre> <p>The command above will create a new bucket in the specified AWS region if it does not already exist. The created bucket will serve as the remote cache for K8sGPT.</p> <p>Listing Cache Items:</p> <p>To view the items stored in the remote cache, users can use the following command:</p> <pre><code>k8sgpt cache list\n</code></pre> <p>Removing the Remote Cache:</p> <p>If users wish to remove the remote cache without deleting the associated AWS S3 bucket, they can use the following command:</p> <pre><code>k8sgpt cache remove --bucket &lt;name&gt;\n</code></pre> <p>This command ensures that the cache items are removed from the K8sGPT CLI, but the bucket and its contents in AWS S3 will remain intact for potential future usage.</p>"},{"location":"explanation/integrations/","title":"Integrations","text":"<p>Integrations in k8sGPT allows you to manage and configure various integrations with external tools and services within your repository's codebase.</p> <p>These Integrations enhance the functionality of k8sGPT by providing additional capabilities for scanning, diagnosing, and triaging issues in the Kubernetes clusters.</p>"},{"location":"explanation/integrations/#description","title":"Description","text":"<p>The <code>integration</code> command in the k8sgpt enables seamless integration with external tools and services. It allows you to activate, configure, and manage integrations that complement the functionalities of k8sgpt.</p> <p>Integrations are designed to interact with external systems and tools that complement the functionalities of k8sgpt. These integrations include vulnerability scanners, monitoring services, incident management platforms, and more</p> <p>By using the following command users can access all K8sGPT CLI options related to integrations:</p> <pre><code>k8sgpt integrations --help\n</code></pre> <p>By leveraging the <code>integration</code> feature in the K8sGPT CLI, users can extend the functionality of K8sGPT by incorporating various external tools and services. This collaboration enhances the ability to diagnose and triage issues in Kubernetes clusters more effectively.</p> <p>For more information about each <code>integration</code> and its specific configuration options, refer to the reference documentation provided for the integration.</p>"},{"location":"getting-started/Community/","title":"Community","text":""},{"location":"getting-started/Community/#github","title":"GitHub","text":"<p>the k8sgpt source code and other related projects managed on github are in the k8sgpt organization</p>"},{"location":"getting-started/Community/#slack-channel","title":"Slack Channel","text":"<p>You can join the slack channel using the  link : slack</p>"},{"location":"getting-started/Community/#community-meetings-office-hours","title":"Community Meetings / Office Hours","text":"<p>these happen on 1st and 3rd Thursday of the month Time zone: Europe/London Time: 12:00 - 13:00 Joining Info:</p> <p>Google Meet : link</p> <p>Calendar Link : calendar schedule</p>"},{"location":"getting-started/Community/#contributing","title":"Contributing","text":"<p>Thanks for your interest in contributing! We welcome all types of contributions and encourage you to read our contribution guidelines for next steps.</p>"},{"location":"getting-started/getting-started/","title":"Getting Started Guide","text":"<p>You can either get started with K8sGPT in your own environment, the details are provided below or you can use our Playground example on Killrcoda.</p> <p>Tip</p> <p>Please only use K8sGPT on environments where you are authorized to modify Kubernetes resources.</p>"},{"location":"getting-started/getting-started/#prerequisites","title":"Prerequisites","text":"<ol> <li>Ensure <code>k8sgpt</code> is installed correctly on your environment by following the installation.</li> <li>You need to be connected to any Kubernetes cluster. Below is the documentation for setting up a new KinD Kubernetes cluster. However, make sure that kubectl is already installed.</li> </ol>"},{"location":"getting-started/getting-started/#setting-up-a-kubernetes-cluster","title":"Setting up a Kubernetes cluster","text":"<p>To give <code>k8sgpt</code> a try, set up a basic Kubernetes cluster, such as KinD or Minikube (if you are not connected to any other cluster).</p> <ul> <li> <p>The KinD documentation provides several installation options to set up a local cluster with two commands.</p> </li> <li> <p>The Minikube documentation covers different Operating Systems and Architectures to set up a local Kubernetes cluster running on a Container or Virtual Machine.</p> </li> </ul> <p>Creating a KinD Kubernetes Cluster</p> <p>Install KinD first:</p> <pre><code>brew install kind\n</code></pre> <p>Create a new Kubernetes cluster:</p> <pre><code>kind create cluster --name k8sgpt-demo\n</code></pre>"},{"location":"getting-started/getting-started/#using-k8sgpt","title":"Using K8sGPT","text":"<p>You can view the different command options through </p> <pre><code>k8sgpt --help\nKubernetes debugging powered by AI\n\nUsage:\n  k8sgpt [command]\n\nAvailable Commands:\n  analyze     This command will find problems within your Kubernetes cluster\n  auth        Authenticate with your chosen backend\n  cache       For working with the cache the results of an analysis\n  completion  Generate the autocompletion script for the specified shell\n  filters     Manage filters for analyzing Kubernetes resources\n  generate    Generate Key for your chosen backend (opens browser)\n  help        Help about any command\n  integration Integrate another tool into K8sGPT\n  serve       Runs k8sgpt as a server\n  version     Print the version number of k8sgpt\n\nFlags:\n      --config string        Default config file (default is $HOME/.k8sgpt.yaml)\n  -h, --help                 help for k8sgpt\n      --kubeconfig string    Path to a kubeconfig. Only required if out-of-cluster.\n      --kubecontext string   Kubernetes context to use. Only required if out-of-cluster.\n\nUse \"k8sgpt [command] --help\" for more information about a command.\n</code></pre>"},{"location":"getting-started/getting-started/#authenticate-with-openai","title":"Authenticate with OpenAI","text":"<p>First, you will need to authenticate with your chosen backend. The backend is the AI provider such as OpenAI's ChatGPT.</p> <p>Ensure that you have created an account with OpenAI.</p> <p>Next, generate a token from the backend:</p> <pre><code>k8sgpt generate\n</code></pre> <p>This will provide you with a URL to generate a token, follow the URL from the command line to your browser to then generate the token.</p> <p></p> <p>Copy the token for the next step.</p> <p>Then, authenticate with the following command:</p> <pre><code>k8sgpt auth add --backend openai --model gpt-3.5-turbo\n</code></pre> <p>This will request the token that has just been generated. Paste the token into the command line.</p> <p>You should then see the following success message:</p> <p>Enter openai Key: openai added to the AI backend provider list</p>"},{"location":"getting-started/getting-started/#analyze-your-cluster","title":"Analyze your cluster","text":"<p>Ensure that you are connected the correct Kubernetes cluster, for this initial example is preferable to use KinD or Minikube as discussed earlier.</p> <pre><code>kubectl config current-context\n</code></pre> <pre><code>kubectl get nodes\n</code></pre> <p>We will now create a new \"broken Pod\", simply create a new YAML file named <code>broken-pod.yml</code> with the following contents:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pod\n  namespace: default\nspec:\n  containers:\n    - name: broken-pod\n      image: nginx:1.a.b.c\n      livenessProbe:\n        httpGet:\n          path: /\n          port: 81\n        initialDelaySeconds: 3\n        periodSeconds: 3\n</code></pre> <p>You might have noticed, this Pod has a wrong image tag. This is ok for this example, we simply want to have an issue in our cluster. The simply run:</p> <pre><code>kubectl apply -f broken-pod.yml\n</code></pre> <p>This will create the \"broken Pod\" in the cluster. You can verify this by running:</p> <pre><code>kubectl get pods\n\nNAME         READY   STATUS         RESTARTS   AGE\nbroken-pod   0/1     ErrImagePull   0          5s\n</code></pre> <p>Now, you can go ahead and analyse your cluster:</p> <pre><code>k8sgpt analyze\n</code></pre> <p>Executing this command will generate a list of issues present in your Kubernetes cluster. In the case of our example, a message should be displayed highlighting the problem related to the container image.</p> <pre><code>0 default/broken-pod(broken-pod)\n- Error: Back-off pulling image \"nginx:1.a.b.c\"\n</code></pre> <p>Info</p> <p>To become acquainted with the available flags supported by the <code>analyse</code> command, type <code>k8sgpt analyse -h</code> for more information. This will provide you with a comprehensive list of all the flags that can be utilized.</p> <p>For a more engaging experience and a better understanding of the capabilities of <code>k8sgpt</code> and LLMs (Large Language Models), run the following command:</p> <pre><code>k8sgpt analyse --explain\n</code></pre> <p>Congratulations! you have successfully created a local kubernetes cluster, deployed a \"broken Pod\" and analyzed it using <code>k8sgpt</code>.</p>"},{"location":"getting-started/in-cluster-operator/","title":"K8sGPT Operator","text":""},{"location":"getting-started/in-cluster-operator/#prerequisites","title":"Prerequisites","text":"<ul> <li>To begin you will require a Kubernetes cluster available and <code>KUBECONFIG</code> set.</li> <li>You will also need to install helm v3. See the Helm documentation for more information.</li> </ul>"},{"location":"getting-started/in-cluster-operator/#operator-installation","title":"Operator Installation","text":"<p>To install the operator, run the following command:</p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n</code></pre> <p>This will install the Operator into the cluster, which will await a <code>K8sGPT</code> resource before anything happens.</p>"},{"location":"getting-started/in-cluster-operator/#deploying-an-openai-secret","title":"Deploying an OpenAI secret","text":"<p>Whilst there are multiple backends supported ( OpenAI, Azure OpenAI and Local ), in this example we'll use OpenAI. Whatever backend you are using, you need to make sure to have a secret that works with the backend.</p> <p>For instance, this means you will need to install your OpenAI token as a secret into the cluster:</p> <pre><code>kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system\n</code></pre>"},{"location":"getting-started/in-cluster-operator/#deploying-a-k8sgpt-resource","title":"Deploying a K8sGPT resource","text":"<p>To deploy a K8sGPT resource, you will need to create a YAML file with the following contents:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-sample\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    model: gpt-3.5-turbo\n    backend: openai\n    secret:\n      name: k8sgpt-sample-secret\n      key: openai-api-key\n    # anonymized: false\n    # language: english\n  noCache: false\n  version: v0.3.17\n  # filters:\n  #   - Ingress\n  # sink:\n  #   type: slack\n  #   webhook: &lt;webhook-url&gt;\n  # extraOptions:\n  #   backstage:\n  #     enabled: true\nEOF\n</code></pre> <p>Please replace the <code>&lt;VERSION&gt;</code> field with the current release of K8sGPT. At the time of writing this is <code>v0.3.17</code>.</p>"},{"location":"getting-started/in-cluster-operator/#regarding-out-of-cluster-traffic-to-ai-backends","title":"Regarding out-of-cluster traffic to AI backends","text":"<p>In the above example <code>enableAI</code> is set to <code>true</code>. This option allows the cluster deployment to use the <code>backend</code> to filter and improve the responses to the user. Those responses will appear as <code>details</code> within the <code>Result</code> custom resources that are created.</p> <p>The default backend in this example is OpenAI and allows for additional details to be generated and solutions provided for issues. If you wish to disable out-of-cluster communication and any Artificial Intelligence processing through models, simply set <code>enableAI</code> to <code>false</code>.</p> <p>It should also be noted that <code>localai</code> and <code>azureopenai</code> is supported and in-cluster models will be supported in the near future</p>"},{"location":"getting-started/in-cluster-operator/#viewing-the-results","title":"Viewing the results","text":"<p>Once the initial scans have been completed after several minutes, you will be presented with results custom resources.</p> <pre><code>\u276f kubectl get results -o json | jq .\n{\n  \"apiVersion\": \"v1\",\n  \"items\": [\n    {\n      \"apiVersion\": \"core.k8sgpt.ai/v1alpha1\",\n      \"kind\": \"Result\",\n      \"metadata\": {\n        \"creationTimestamp\": \"2023-04-26T09:45:02Z\",\n        \"generation\": 1,\n        \"name\": \"placementoperatorsystemplacementoperatorcontrollermanagermetricsservice\",\n        \"namespace\": \"default\",\n        \"resourceVersion\": \"108371\",\n        \"uid\": \"f0edd4de-92b6-4de2-ac86-5bb2b2da9736\"\n      },\n      \"spec\": {\n        \"details\": \"The error message means that the service in Kubernetes doesn't have any associated endpoints, which should have been labeled with \\\"control-plane=controller-manager\\\". \\n\\nTo solve this issue, you need to add the \\\"control-plane=controller-manager\\\" label to the endpoint that matches the service. Once the endpoint is labeled correctly, Kubernetes can associate it with the service, and the error should be resolved.\",\n        ...\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This page provides further information on installation guidelines.</p>"},{"location":"getting-started/installation/#linuxmac-via-brew","title":"Linux/Mac via brew","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have Homebrew installed:</p> <ul> <li>Homebrew for Mac</li> <li>Homebrew for Linux Homebrew for Linux also works on WSL</li> </ul>"},{"location":"getting-started/installation/#homebrew","title":"Homebrew","text":"<p>Install K8sGPT on your machine with the following commands:</p> <pre><code>brew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n</code></pre>"},{"location":"getting-started/installation/#other-installation-options","title":"Other Installation Options","text":""},{"location":"getting-started/installation/#rpm-based-installation-redhatcentosfedora","title":"RPM-based installation (RedHat/CentOS/Fedora)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.rpm\nsudo rpm -ivh k8sgpt_386.rpm\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.rpm\nsudo rpm -ivh -i k8sgpt_amd64.rpm\n</code></pre>"},{"location":"getting-started/installation/#deb-based-installation-ubuntudebian","title":"DEB-based installation (Ubuntu/Debian)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.deb\nsudo dpkg -i k8sgpt_386.deb\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.deb\nsudo dpkg -i k8sgpt_amd64.deb\n</code></pre>"},{"location":"getting-started/installation/#apk-based-installation-alpine","title":"APK-based installation (Alpine)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.apk\napk add k8sgpt_386.apk\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.apk\napk add k8sgpt_amd64.apk\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ul> <li>Download the latest Windows binaries of k8sgpt from the Release    tab based on your system architecture.</li> <li>Extract the downloaded package to your desired location. Configure the system path variable with the binary location</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify installation","text":"<p>Verify that K8sGPT is installed correctly:</p> <pre><code>k8sgpt version\n\nk8sgpt version 0.2.7\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#windows-wsl","title":"Windows WSL","text":"<p>Failing Installation on WSL or Linux (missing gcc) When installing Homebrew on WSL or Linux, you may encounter the following error:</p> <pre><code>==&gt; Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from bottle and must be \nbuilt from source. k8sgpt Install Clang or run brew install gcc.\n</code></pre> <p>If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.</p> <pre><code>   sudo apt-get update\n   sudo apt-get install build-essential\n</code></pre>"},{"location":"getting-started/installation/#failing-installation-on-wsl-or-linux-missing-gcc","title":"Failing Installation on WSL or Linux (missing gcc)","text":"<p>When installing Homebrew on WSL or Linux, you may encounter the following error:</p> <p><code>==&gt; Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be    built from the source. k8sgpt Install Clang or run brew install gcc.</code></p> <p>If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.   <code>bash      sudo apt-get update      sudo apt-get install build-essential</code></p>"},{"location":"getting-started/installation/#running-k8sgpt-through-a-container","title":"Running K8sGPT through a container","text":"<p>If you are running K8sGPT through a container, the CLI will not be able to open the website for the OpenAI token.</p> <p>You can find the latest container image for K8sGPT in the packages of the GitHub organisation: Link</p> <p>A volume can then be mounted to the image through e.g. Docker Compose. Below is an example:</p> <pre><code>version: '2'\nservices:\n k8sgpt:\n   image: ghcr.io/k8sgpt-ai/k8sgpt:dev-202304011623\n   volumes:\n     -  /home/$(whoami)/.k8sgpt.yaml:/home/root/.k8sgpt.yaml\n</code></pre>"},{"location":"getting-started/installation/#installing-the-k8sgpt-operator-helm-chart","title":"Installing the K8sGPT Operator Helm Chart","text":"<p>K8sGPT can be installed as an Operator inside the cluster.  For further information, see the K8sGPT Operator documentation.</p>"},{"location":"getting-started/installation/#upgrading-the-brew-installation","title":"Upgrading the brew installation","text":"<p>To upgrade the K8sGPT brew installation run the following command:</p> <pre><code>brew upgrade k8sgpt\n</code></pre>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>This section provides an overview of the different <code>k8sgpt</code> CLI commands.</p> <p>Prerequisites</p> <ul> <li>You need to be connected to a Kubernetes cluster, K8sGPT will access it through your kubeconfig.</li> <li>Signed-up to OpenAI ChatGPT</li> <li>Have the K8sGPT CLI installed</li> </ul>"},{"location":"reference/cli/#commands","title":"Commands","text":"<p>Run a scan with the default analyzers</p> <pre><code>k8sgpt generate\nk8sgpt auth new\nk8sgpt analyze --explain\n</code></pre> <p>Filter on resource</p> <pre><code>k8sgpt analyze --explain --filter=Service\n</code></pre> <p>Filter by namespace</p> <pre><code>k8sgpt analyze --explain --filter=Pod --namespace=default\n</code></pre> <p>Output to JSON</p> <pre><code>k8sgpt analyze --explain --filter=Service --output=json\n</code></pre> <p>Anonymize during explain</p> <pre><code>k8sgpt analyze --explain --filter=Service --output=json --anonymize\n</code></pre>"},{"location":"reference/cli/#additional-commands","title":"Additional commands","text":"<p>List configured backends</p> <pre><code>k8sgpt auth list\n</code></pre> <p>Remove configured backends</p> <pre><code>k8sgpt auth remove --backend $MY_BACKEND\n</code></pre> <p>List integrations</p> <pre><code>k8sgpt integrations list\n</code></pre> <p>Activate integrations</p> <pre><code>k8sgpt integrations activate [integration(s)]\n</code></pre> <p>Use integration</p> <pre><code>k8sgpt analyze --filter=[integration(s)]\n</code></pre> <p>Deactivate integrations</p> <pre><code>k8sgpt integrations deactivate [integration(s)]\n</code></pre> <p>Serve mode</p> <pre><code>k8sgpt serve\n</code></pre> <p>Analysis with serve mode</p> <pre><code>curl -X GET \"http://localhost:8080/analyze?namespace=k8sgpt&amp;explain=false\"\n</code></pre>"},{"location":"reference/cli/filters/","title":"Using Integration and Filters in K8sGPT","text":"<p>K8sGPT offers integration with other tools. Once an integration is added to K8sGPT, it is possible to use its resources as additional filters.</p> <ul> <li>Filters are a way of selecting which resources you wish to be part of your default analysis.</li> <li>Integrations are a way to add resources to the filter list.</li> </ul> <p>The first integration that has been added is Trivy. Trivy is an open source, cloud native security scanner, maintained by Aqua Security.</p> <p>K8sGPT also supports a Prometheus integration. Prometheus is an open source monitoring solution.</p> <p>Use the following command to access all K8sGPT CLI options related to integrations:</p> <pre><code>k8sgpt integrations\n</code></pre>"},{"location":"reference/cli/filters/#prerequisites","title":"Prerequisites","text":"<p>For using the K8sGPT integrations please ensure that you have the latest version of the K8sGPT CLI installed. Also, please make sure that you are connected to a Kubernetes cluster.</p>"},{"location":"reference/cli/filters/#activating-a-new-integration","title":"Activating a new integration","text":"<p>Prerequisites</p> <ul> <li>Connected to a running Kubernetes cluster, any cluster will work for demonstration purposes</li> </ul> <p>To list all integrations run the following command:</p> <pre><code>k8sgpt integrations list\n</code></pre> <p>This will provide you with a list of available integrations.</p>"},{"location":"reference/cli/filters/#trivy","title":"Trivy","text":"<p>Activate the Trivy integration:</p> <pre><code>k8sgpt integration activate trivy\n</code></pre> <p>Once activated, you should see the following success message displayed:</p> <pre><code>Activated integration trivy\n</code></pre> <p>This will install the Trivy Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.</p> <p>Once the Trivy Operator is installed inside the cluster, K8sGPT will have access to VulnerabilityReports and ConfigAuditReports:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; VulnerabilityReport (integration)\n&gt; Pod\n&gt; ConfigAuditReport (integration)\nUnused:\n&gt; PersistentVolumeClaim\n&gt; Service\n&gt; CronJob\n&gt; Node\n&gt; MutatingWebhookConfiguration\n&gt; Deployment\n&gt; StatefulSet\n&gt; ValidatingWebhookConfiguration\n&gt; ReplicaSet\n&gt; Ingress\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n</code></pre> <p>More information can be found on the official Trivy-Operator documentation.</p>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>To use the <code>VulnerabilityReport</code> filter from the Trivy integration, set it through the <code>--filter</code> flag:</p> <pre><code>k8sgpt analyze --filter VulnerabilityReport\n</code></pre> <p>This command will analyze your cluster Vulnerabilities through K8sGPT. Depending on the VulnerabilityReports available in your cluster, the result of the report will look different:</p> <pre><code>\u276f k8sgpt analyze --filter VulnerabilityReport\n\n0 demo/nginx-deployment-7bcfc88bbf(Deployment/nginx-deployment)\n- Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914)\n- Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536)\n- Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914)\n- Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536)\n- Error: critical Vulnerability found ID: CVE-2019-8457 (learn more at: https://avd.aquasec.com/nvd/cve-2019-8457)\n</code></pre>"},{"location":"reference/cli/filters/#prometheus","title":"Prometheus","text":"<p>The Prometheus integration does not deploy resources in your cluster. Instead, it detects a running Prometheus stack in the provided namespace using the <code>--namespace</code> flag. If you do not have Prometheus running, you can install it using prometheus-operator or kube-prometheus.</p> <p>Activate the Prometheus integration:</p> <pre><code>k8sgpt integration activate prometheus --namespace &lt;namespace&gt;\n</code></pre> <p>If successful, you should see the following success message displayed:</p> <pre><code>Activating prometheus integration...\nFound existing installation\nActivated integration prometheus\n</code></pre> <p>Otherwise, it will report an error:</p> <pre><code>Activating prometheus integration...\nPrometheus installation not found in namespace: &lt;namespace&gt;.\n                Please ensure Prometheus is deployed to analyze.\nError: no prometheus installation found\n</code></pre> <p>Once activated, K8sGPT will have access to new filters:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; PersistentVolumeClaim\n&gt; Service\n&gt; ValidatingWebhookConfiguration\n&gt; MutatingWebhookConfiguration\n&gt; PrometheusConfigRelabelReport (integration)\n&gt; Deployment\n&gt; CronJob\n&gt; Node\n&gt; Pod\n&gt; PrometheusConfigValidate (integration)\n&gt; Ingress\n&gt; StatefulSet\n&gt; PrometheusConfigReport\n&gt; ReplicaSet\nUnused:\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n&gt; Log\n&gt; GatewayClass\n&gt; Gateway\n&gt; HTTPRoute\n</code></pre>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster_1","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>The <code>PrometheusConfigValidate</code> analyzer does a basic \"sanity-check\" on your Prometheus configuration to ensure it is formatted correctly and that Prometheus can load it properly. For example, if Prometheus is deployed in the <code>monitoring</code> namespace and has a bad config, we can analyze the issue using the <code>--filter</code> flag:</p> <pre><code>\u276f k8sgpt analyze --filter PrometheusConfigValidate --namespace monitoring --explain\n\n0 monitoring/prometheus-test-0(StatefulSet/prometheus-test)\n- Error: error validating Prometheus YAML configuration: unknown relabel action \"keeps\"\nError: Unknown relabel action \"keeps\" in Prometheus configuration.\n\nSolution:\n1. Check the Prometheus documentation for valid relabel actions.\n2. Correct the relabel action to a valid one, such as \"keep\" or \"drop\".\n3. Ensure the relabel configuration is correct and matches the intended behavior.\n4. Restart Prometheus to apply the changes.\n</code></pre> <p>The <code>PrometheusConfigRelabelReport</code> analyzer parses your Prometheus relabeling rules and reports groups of labels needed by your targets to be scraped successfully.</p> <pre><code>\u276f k8sgpt analyze --filter PrometheusConfigRelabelReport --namespace monitoring --explain\n\nDiscovered and parsed Prometheus scrape configurations.\nFor targets to be scraped by Prometheus, ensure they are running with\nat least one of the following label sets:\n- Job: prom-example\n  - Service Labels:\n    - app.kubernetes.io/name=prom-example\n  - Pod Labels:\n    - app.kubernetes.io/name=prom-example\n  - Namespaces:\n    - default\n  - Ports:\n    - metrics\n  - Containers:\n    - prom-example\n- Job: collector\n  - Service Labels:\n    - app.kubernetes.io/name=collector\n  - Pod Labels:\n    - app.kubernetes.io/name=collector\n  - Namespaces:\n    - monitoring\n  - Ports:\n    - prom-metrics\n  - Containers:\n    - collector\n</code></pre> <p>Note: the LLM prompt includes a subset of your Prometheus relabeling rules to avoid using too many tokens, so you may not see every label set in the output.</p>"},{"location":"reference/cli/filters/#aws","title":"AWS","text":"<p>The AWS Operator is a tool that allows Kubernetes to manage AWS resources directly, making it easier to integrate AWS services with other Kubernetes applications. This integration helps K8sGPT to interact with the AWS resources managed by the Operator. As a result, you can use K8sGPT to analyze and manage not only your Kubernetes resources but also your AWS resources that are under the management of the AWS Operator.</p> <p>Activate the AWS integration:</p> <pre><code>k8sgpt integration activate aws\n</code></pre> <p>Once activated, you should see the following success message displayed:</p> <pre><code>Activated integration aws\n</code></pre> <p>This will activate the AWS Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.</p> <p>Once the AWS integration is activated inside the cluster, K8sGPT will have access to EKS:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; StatefulSet\n&gt; Ingress\n&gt; Pod\n&gt; Node\n&gt; ValidatingWebhookConfiguration\n&gt; Service\n&gt; EKS (integration)\n&gt; PersistentVolumeClaim\n&gt; MutatingWebhookConfiguration\n&gt; CronJob\n&gt; Deployment\n&gt; ReplicaSet\nUnused:\n&gt; Log\n&gt; GatewayClass\n&gt; Gateway\n&gt; HTTPRoute\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n</code></pre> <p>More information can be found on the official AWS-Operator documentation.</p>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster_2","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>Note: Ensure the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables are set as outlined in the AWS CLI environment variables documentation.</p> <p>To use the <code>EKS</code> filter from the AWS integration, specify it with the --filter flag:</p> <pre><code>k8sgpt analyze --filter EKS\n</code></pre> <p>This command analyzes your cluster's EKS resources using K8sGPT. Make sure your EKS cluster is working in the specified namespace. The report's results will vary based on the EKS reports available in your cluster.</p>"},{"location":"reference/cli/filters/#adding-and-removing-default-filters","title":"Adding and removing default filters","text":"<p>Remove default filters</p> <pre><code>k8sgpt filters add [filter(s)]\n</code></pre> <ul> <li>Simple filter : <code>k8sgpt filters add Service</code></li> <li>Multiple filters : <code>k8sgpt filters add Ingress,Pod</code></li> </ul> <p>Remove default filters</p> <pre><code>k8sgpt filters remove [filter(s)]\n</code></pre> <ul> <li>Simple filter : <code>k8sgpt filters remove Service</code></li> <li>Multiple filters : <code>k8sgpt filters remove Ingress,Pod</code></li> </ul>"},{"location":"reference/cli/serve-mode/","title":"K8sGPT Serve mode","text":""},{"location":"reference/cli/serve-mode/#prerequisites","title":"Prerequisites","text":"<ol> <li>Have grpcurl installed</li> </ol>"},{"location":"reference/cli/serve-mode/#run-k8sgpt-serve-mode","title":"Run <code>k8sgpt</code> serve mode","text":"<pre><code>$ k8sgpt serve\n{\"level\":\"info\",\"ts\":1684309627.113916,\"caller\":\"server/server.go:83\",\"msg\":\"binding metrics to 8081\"}\n{\"level\":\"info\",\"ts\":1684309627.114198,\"caller\":\"server/server.go:68\",\"msg\":\"binding api to 8080\"}\n</code></pre> <p>This command starts two servers:</p> <ol> <li>The health server runs on port 8081 by default and serves metrics and health endpoints.</li> <li>The API server runs on port 8080 (gRPC) by default and serves the analysis handler.</li> </ol> <p>For more details about the gRPC implementation, refer to this link.</p>"},{"location":"reference/cli/serve-mode/#analyze-your-cluster-with-grpcurl","title":"Analyze your cluster with <code>grpcurl</code>","text":"<p>Make sure you are connected to a Kubernetes cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Next, run the following command:</p> <pre><code>grpcurl -plaintext localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>This command provides a list of issues in your Kubernetes cluster. If there are no issues identified, you should receive a status of <code>OK</code>.</p>"},{"location":"reference/cli/serve-mode/#analyze-with-parameters","title":"Analyze with parameters","text":"<p>You can specify parameters using the following command:</p> <pre><code>grpcurl -plaintext -d '{\"explain\": false, \"filters\": [\"Ingress\"], \"namespace\": \"k8sgpt\"}' localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>In this example, the analyzer will only consider the <code>k8sgpt</code> namespace without AI explanation and only focus on the <code>Ingress</code> filter.</p>"},{"location":"reference/guidelines/community/","title":"Community Information","text":"<p>All community related information are kept in a separate repository from the docs.</p> <p>Link to the repository: k8sgpt-ai/community</p> <p>There you will find information on</p> <ul> <li>The Charter</li> <li>Adopters List</li> <li>Code of Conduct</li> <li>Community Members</li> <li>Subprojects</li> </ul> <p>and much more.</p>"},{"location":"reference/guidelines/guidelines/","title":"Contributing Guidelines","text":""},{"location":"reference/guidelines/guidelines/#contributing-to-the-documentation","title":"Contributing to the Documentation","text":"<p>This documentation follows the Diataxis framework. If you are proposing completely new content to the documentation, please familiarise yourself with the framework first.</p> <p>The documentation is created with mkdocs, specifically the Material for MkDocs theme.</p>"},{"location":"reference/guidelines/guidelines/#contributing-projects-in-the-k8sgpt-organisation","title":"Contributing projects in the K8sGPT organisation","text":"<p>All project in the K8sGPT organisation follow our contributing guidelines.</p>"},{"location":"reference/guidelines/privacy/","title":"Privacy","text":"<p>K8sGPT is a privacy-first tool and believe transparency is key for you to understand how we use your data. We have created this page to help you understand how we collect, use, share and protect your data.</p>"},{"location":"reference/guidelines/privacy/#data-we-collect","title":"Data we collect","text":"<p>K8sGPT will collect data from Analyzers and either display it directly to you or  with the <code>--explain</code> flag it will send it to the selected AI backend.</p> <p>The type of data collected depends on the Analyzer you are using. For example, the <code>k8sgpt analyze pod</code> command will collect the following data: - Container status message - Pod name - Pod namespace - Event message</p>"},{"location":"reference/guidelines/privacy/#data-we-share","title":"Data we share","text":"<p>As mentioned, K8sGPT will share data with the selected AI backend only when you choose <code>--explain</code> and <code>auth</code> against that backend. The data shared will be the same as the data collected.</p> <p>To learn more about the privacy policy of our default AI backend OpenAI please visit their privacy policy.</p>"},{"location":"reference/guidelines/privacy/#data-we-protect","title":"Data we protect","text":"<p>When you are sending data through the <code>--explain</code> option, there is the capability of anonymising some of that data. This is done by using the <code>--anonymize</code> flag. In the example of the Deployment Analyzer, this will obfusicate the following data:</p> <ul> <li>Deployment name</li> <li>Deployment namespace</li> </ul>"},{"location":"reference/guidelines/privacy/#data-we-dont-collect","title":"Data we don't collect","text":"<ul> <li>Logs</li> <li>API Server data other than the primitives used within our Analyzers.</li> </ul>"},{"location":"reference/guidelines/privacy/#contact","title":"Contact","text":"<p>If you have any questions about our privacy policy, please contact us.</p>"},{"location":"reference/operator/advanced-installation/","title":"Advanced Operator installation options","text":"<p>This documentation lists advanced installation options for the K8sGPT Operator.</p>"},{"location":"reference/operator/advanced-installation/#argocd","title":"ArgoCD","text":"<p>ArgoCD is a continuous Deployment tool that implements GitOps best practices to install and manage Kubernetes resources.</p>"},{"location":"reference/operator/advanced-installation/#prerequisites","title":"Prerequisites","text":"<p>To install and manage K8sGPT through ArgoCD, ensure that you have ArgoCD installed and running inside your cluster. The ArgoCD getting-started-guide provides detailed information.</p>"},{"location":"reference/operator/advanced-installation/#installing-k8sgpt","title":"Installing K8sGPT","text":"<p>K8sGPT can be installed through ArgoCD by applying an <code>Application</code> CRD to the ArgoCD namespaces in your cluster (with ArgoCD running):</p> <p>K8sGPT Application CRD:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: k8sgpt\n  namespace: argocd\nspec:\n  project: default\n  source:\n    chart: k8sgpt-operator\n    repoURL: https://charts.k8sgpt.ai/\n    targetRevision: &lt;VERSION&gt;\n    helm:\n      values: |\n        serviceMonitor:\n          enabled: true\n        GrafanaDashboard:\n          enabled: true\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: k8sgpt-operator-system\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Note: </p> <ul> <li>Ensure that the <code>namespace</code> is correctly set to your ArgoCD namespace</li> <li>Ensure that the <code>&lt;VERSION&gt;</code> is set to the K8sGPT Operator Release Version that you want to use.</li> <li>Modify the <code>helm.values</code> section with the Helm Values that you would like to overwrite. Check the values.yaml file of the Operator for options.</li> </ul> <p>Applying the resource:</p> <pre><code>kubectl apply -f application.yaml\n</code></pre>"},{"location":"reference/operator/advanced-installation/#installing-the-remaining-operator-resources","title":"Installing the remaining Operator resources","text":"<p>You will still need to install the</p> <ul> <li>K8sGPT Operator CRD </li> <li>K8sGPT secret to access the AI backend</li> </ul> <p>that are both detailed in the Operator installation page. The above Application resource will only install the Operator pods themselves not additional resources. Note that you could manage those resources also through ArgoCD. Please refer to the official ArgoCD documentation for further information.</p>"},{"location":"reference/operator/overview/","title":"K8sGPT Operator","text":"<p>K8sGPT can run as a Kubernetes Operator inside the cluster. The scan results are provided as Kubernetes YAML manifests.</p> <p>This section will only detail how to configure the operator. For installation instructions, please see the getting-started section.</p>"},{"location":"reference/operator/overview/#architecture","title":"Architecture","text":"<p>The diagram below showcases the different components that the K8sGPT Operator installs and manages:</p> <p></p>"},{"location":"reference/operator/overview/#customising-the-operator","title":"Customising the Operator","text":"<p>As with other Helm Charts, the K8sGPT Operator can be customised by modifying  the  <code>values.yaml</code> file.</p> <p>The following fields can be customised in the Helm Chart Deployment:</p> Parameter Description Default <code>serviceMonitor.enabled</code> <code>false</code> <code>serviceMonitor.additionalLabels</code> <code>{}</code> <code>grafanaDashboard.enabled</code> <code>false</code> <code>grafanaDashboard.folder.annotation</code> <code>\"grafana_folder\"</code> <code>grafanaDashboard.folder.name</code> <code>\"ai\"</code> <code>grafanaDashboard.label.key</code> <code>\"grafana_dashboard\"</code> <code>grafanaDashboard.label.value</code> <code>\"1\"</code> <code>controllerManager.kubeRbacProxy.containerSecurityContext.allowPrivilegeEscalation</code> <code>false</code> <code>controllerManager.kubeRbacProxy.containerSecurityContext.capabilities.drop</code> <code>[\"ALL\"]</code> <code>controllerManager.kubeRbacProxy.image.repository</code> <code>\"gcr.io/kubebuilder/kube-rbac-proxy\"</code> <code>controllerManager.kubeRbacProxy.image.tag</code> <code>\"v0.0.19\"</code> <code>controllerManager.kubeRbacProxy.resources.limits.cpu</code> <code>\"500m\"</code> <code>controllerManager.kubeRbacProxy.resources.limits.memory</code> <code>\"128Mi\"</code> <code>controllerManager.kubeRbacProxy.resources.requests.cpu</code> <code>\"5m\"</code> <code>controllerManager.kubeRbacProxy.resources.requests.memory</code> <code>\"64Mi\"</code> <code>controllerManager.manager.sinkWebhookTimeout</code> <code>\"30s\"</code> <code>controllerManager.manager.containerSecurityContext.allowPrivilegeEscalation</code> <code>false</code> <code>controllerManager.manager.containerSecurityContext.capabilities.drop</code> <code>[\"ALL\"]</code> <code>controllerManager.manager.image.repository</code> <code>\"ghcr.io/k8sgpt-ai/k8sgpt-operator\"</code> <code>controllerManager.manager.image.tag</code> <code>\"v0.0.19\"</code> <code>controllerManager.manager.resources.limits.cpu</code> <code>\"500m\"</code> <code>controllerManager.manager.resources.limits.memory</code> <code>\"128Mi\"</code> <code>controllerManager.manager.resources.requests.cpu</code> <code>\"10m\"</code> <code>controllerManager.manager.resources.requests.memory</code> <code>\"64Mi\"</code> <code>controllerManager.replicas</code> <code>1</code> <code>kubernetesClusterDomain</code> <code>\"cluster.local\"</code> <code>metricsService.ports</code> <code>[{\"name\": \"https\", \"port\": 8443, \"protocol\": \"TCP\", \"targetPort\": \"https\"}]</code> <code>metricsService.type</code> <code>\"ClusterIP\"</code>"},{"location":"reference/operator/overview/#for-example-in-cluster-metrics","title":"For example: In-cluster metrics","text":"<p>It is possible to enable metrics of the operator so that they can be scraped through Prometheus.</p> <p>This is the configuration required in the <code>values.yaml</code> manifest:</p> <pre><code>serviceMonitor:\n  enabled: true\n</code></pre> <p>The new <code>values.yaml</code> manifest can then be provided upon installing the Operator inside the cluster:</p> <pre><code>helm update --install release k8sgpt/k8sgpt-operator --values values.yaml\n</code></pre>"},{"location":"reference/providers/backend/","title":"K8sGPT AI Backends","text":"<p>A Backend (also called Provider) is a service that provides access to the AI language model. There are many different backends available for K8sGPT. Each backend has its own strengths and weaknesses, so it is important to choose the one that is right for your needs.</p> <p>Currently, we have a total of 8 backends available:</p> <ul> <li>OpenAI</li> <li>Cohere</li> <li>Amazon Bedrock</li> <li>Amazon SageMaker</li> <li>Azure OpenAI</li> <li>Google Gemini</li> <li>LocalAI</li> <li>FakeAI</li> </ul>"},{"location":"reference/providers/backend/#openai","title":"OpenAI","text":"<p>OpenAI is the default backend for K8sGPT. We recommend using OpenAI first if you are new to K8sGPT and if you have an account on OpenAI. OpenAI comes with the access to powerful language models such as GPT-3.5-Turbo, GPT-4. If you are looking for a powerful and easy-to-use language modeling service, OpenAI is a great option.</p> <ul> <li>To use OpenAI you'll need an OpenAI token for authentication purposes. To generate a token use:     <code>bash     k8sgpt generate</code></li> <li>To set the token in K8sGPT, use the following command:     <code>bash     k8sgpt auth add</code></li> <li>Run the following command to analyze issues within your cluster using OpenAI:     <code>bash     k8sgpt analyze --explain</code></li> </ul>"},{"location":"reference/providers/backend/#cohere","title":"Cohere","text":"<p>Cohere allows building conversational apps. It uses Retrieval Augmented Generation (RAG) toolkit that improves LLM's answer accuracy.</p> <ul> <li>To use Cohere, visit Cohere dashboard.</li> <li>To configure backend in K8sGPT, use the following command:     <code>bash     k8sgpt auth add --backend cohere --model command-nightly</code></li> <li>Run the following command to analyze issues within your cluster using Cohere:     <code>bash     k8sgpt analyze --explain --backend cohere</code></li> </ul>"},{"location":"reference/providers/backend/#amazon-bedrock","title":"Amazon Bedrock","text":"<p>Amazon Bedrock allows building and scaling generative AI applications.</p> <ul> <li>To use Bedrock, make sure you have access to Bedrock API and models e.g. in AWS Console you should see something like this:</li> </ul> <p></p> <ul> <li> <p>You will need to set the follow local environmental variables:     ```</p> <ul> <li>AWS_ACCESS_KEY</li> <li>AWS_SECRET_ACCESS_KEY</li> <li>AWS_DEFAULT_REGION ```</li> </ul> </li> <li> <p>To configure backend in K8sGPT use auth command:     <code>bash     k8sgpt auth add --backend amazonbedrock --model anthropic.claude-v2</code></p> </li> <li>Run the following command to analyze issues within your cluster using Amazon Bedrock:     <code>bash     k8sgpt analyze --explain --backend amazonbedrock</code></li> </ul>"},{"location":"reference/providers/backend/#amazon-sagemaker","title":"Amazon SageMaker","text":"<p>The Amazon SageMaker backend allows you to leverage a self-deployed and managed Language Models (LLM) on Amazon SageMaker.</p> <p>Example how to deploy Amazon SageMaker with cdk is available in llm-sagemaker-jumpstart-cdk repo.</p> <ul> <li>To use SageMaker, make sure you have the AWS CLI configured on your machine.</li> <li>You need to have an Amazon SageMaker instance set up.</li> <li>Run the following command to add SageMaker:     <code>bash     k8sgpt auth add --backend amazonsagemaker --providerRegion eu-west-1 --endpointname endpoint-xxxxxxxxxx</code></li> <li>Now you are ready to analyze with the Amazon SageMaker backend:     <code>bash     k8sgpt analyze --explain --backend amazonsagemaker</code></li> </ul>"},{"location":"reference/providers/backend/#azure-openai","title":"Azure OpenAI","text":"<p>Azure OpenAI Provider provides REST API access to OpenAI's powerful language models. It gives the users an advanced language AI with powerful models with the security and enterprise promise of Azure.</p> <ul> <li> <p>The Azure OpenAI Provider requires a deployment as a prerequisite. You can visit their documentation to create your own.   To authenticate with k8sgpt, you would require an Azure OpenAI endpoint of your tenant <code>https://your Azure OpenAI Endpoint</code>,the API key to access your deployment, the Deployment name of your model and the model name itself.</p> </li> <li> <p>Run the following command to authenticate with Azure OpenAI:     <code>bash     k8sgpt auth add --backend azureopenai --baseurl https://&lt;your Azure OpenAI endpoint&gt; --engine &lt;deployment_name&gt; --model &lt;model_name&gt;</code></p> </li> <li>Now you are ready to analyze with the Azure OpenAI backend:     <code>bash     k8sgpt analyze --explain --backend azureopenai</code></li> </ul>"},{"location":"reference/providers/backend/#google-gemini","title":"Google Gemini","text":"<p>Google Gemini allows generative AI capabilities with multimodal approach (it is capable to understand not only text, but also code, audio, image and video). With Gemini models, a new API was introduced, and this is what is now built-in K8sGPT. This API also works against the Google Cloud Vertex AI service. See also Google AI Studio to get started.</p> <p>NOTE: Gemini API might be still rolling to some regions. See the available regions for details.</p> <ul> <li>To use Google Gemini API in K8sGPT, obtain the API key.</li> <li>To configure Google backend in K8sGPT with <code>gemini-pro</code> model (see all models here) use auth command:     <code>bash     k8sgpt auth add --backend google --model gemini-pro --password \"&lt;Your API KEY&gt;\"</code></li> <li>Run the following command to analyze issues within your cluster with the Google provider:     <code>bash     k8sgpt analyze --explain --backend google</code></li> </ul>"},{"location":"reference/providers/backend/#localai","title":"LocalAI","text":"<p>LocalAI is a local model, which is an OpenAI compatible API. It uses llama.cpp and ggml to run inference on consumer-grade hardware. Models supported by LocalAI for instance are Vicuna, Alpaca, LLaMA, Cerebras, GPT4ALL, GPT4ALL-J and koala.</p> <ul> <li>To run local inference, you need to download the models first, for instance you can find <code>ggml</code> compatible models in huggingface.com(for example vicuna, alpaca and koala).</li> <li>To start the API server, follow the instruction in LocalAI.</li> <li>Authenticate K8sGPT with LocalAI:     <code>bash     k8sgpt auth new --backend localai --model &lt;model_name&gt; --baseurl http://localhost:8080/v1</code></li> <li>Analyze with a LocalAI backend:     <code>bash     k8sgpt analyze --explain --backend localai</code></li> </ul>"},{"location":"reference/providers/backend/#fakeai","title":"FakeAI","text":"<p>FakeAI or the NoOpAiProvider might be useful in situations where you need to test a new feature or simulate the behaviour of an AI based-system without actually invoking it. It can help you with local development, testing and troubleshooting. The NoOpAiProvider does not actually perfornm any AI-based operations but simulates them by echoing the input given as a problem.</p> <p>Follow the steps outlined below to learn how to utilize the NoOpAiProvider:</p> <ul> <li>Authorize k8sgpt with <code>noopai</code> or <code>noop</code> as the Backend Provider:     <code>k8sgpt auth -b noopai</code></li> <li> <p>For the auth token, you can leave it blank as the NoOpAiProvider is configured to work fine with or without any token.</p> </li> <li> <p>Use the analyze and explain command to check for errors in your kubernetes cluster and the NoOpAiProvider should return the error as the solution itself:     <code>k8sgpt analyze --explain --backend noopai</code></p> </li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides</p> <ul> <li>end-to-end tutorials on specific use cases</li> <li>a collection of user and contributor created content</li> </ul>"},{"location":"tutorials/playground/","title":"K8sGPT Playground","text":"<p>If you want to try out K8sGPT, we highly suggest you to follow this Killrcoda example:</p> <p>Link: K8sGPT CLI Tutorial</p> <p>This tutorials covers:</p> <ul> <li>Run a simple analysis and explore possible options</li> <li>Discover how AI works Explanation</li> <li>Stay on the down-low with the anonymize option (because we don't want any trouble with the feds)</li> <li>Filter resources like a boss</li> <li>Use Integrations</li> </ul>"},{"location":"tutorials/slack-integration/","title":"Integrate K8sGPT operator with Slack","text":""},{"location":"tutorials/slack-integration/#slack-prerequisites","title":"Slack prerequisites","text":"<ul> <li>Create a slack channel</li> <li>Create a slack app</li> <li>Enable and create an incoming webhook</li> <li>Copy the webhook URL value</li> </ul> <p>You can follow Slack's documentation to create the webhook</p>"},{"location":"tutorials/slack-integration/#configure-the-k8sgpt-operator","title":"Configure the K8sGPT operator","text":"<p>Install the operator with HELM  </p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n</code></pre> <p>Create OpenAI's secret  </p> <pre><code>kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system\n</code></pre> <p>Last but not least, deploy your K8sGPT Custom Resource</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-sample\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    model: gpt-3.5-turbo\n    backend: openai\n    secret:\n      name: k8sgpt-sample-secret\n      key: openai-api-key\n  noCache: false\n  version: v0.3.8\n  sink:\n    type: slack\n    webhook: &lt;your webhook url&gt;\nEOF\n</code></pre>"},{"location":"tutorials/content-collection/content-collection/","title":"Content Collection","text":"<p>This section provides a collection of vidoes, blog posts and more on K8sGPT, posted on external sites.</p>"},{"location":"tutorials/content-collection/content-collection/#blogs","title":"Blogs","text":"<p>Have a look at the K8sGPT blog on the website.</p> <p>Additionally, here are several blogs created by the community:</p> <ul> <li>K8sGPT: The Ultimate Tool for Kubernetes Scanning by Rakshit Gondwal</li> <li>K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! by Tyler Gillson</li> <li>K8sGPT: Simplifying Kubernetes Diagnostics with Natural Language Processing by Karan Singh</li> <li>Kubernetes + ChatGPT = K8sGpt by Vijul Patel</li> <li>ChatGPT for your Kubernetes Cluster \u2014 k8sgpt by Renjith Ravindranathan</li> <li>Using the Trivy K8sGPT plugin by Renjith Ravindranathan</li> </ul>"},{"location":"tutorials/content-collection/content-collection/#videos","title":"Videos","text":"<ul> <li>Kubernetes + OpenAI = K8sGPT, giving you AI superpowers!</li> <li>k8sgpt Getting Started (2023)</li> <li>Debugging Kubernetes with AI: k8sGPT || AI-Powered Debugging for Kubernetes</li> </ul>"},{"location":"tutorials/content-collection/content-collection/#contributing","title":"Contributing","text":"<p>If you have created any content around K8sGPT, then please add it to this collection.</p>"}]}